{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5734edf1-fc84-4487-bddb-441550b103a9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bd9db0b4967463188109c526006cfac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'It seems like you just typed a random word \"test\". Is there something specific you\\'d like to test or discuss? I\\'m here to help with any questions or topics you\\'d like to explore!'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#read data from csv\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import logging\n",
    "logging.basicConfig(level=logging.WARNING)\n",
    "\n",
    "from src.llm_alex import Llama\n",
    "from langchain_core import pydantic_v1\n",
    "from langchain_core.runnables.base import RunnableParallel, RunnableLambda\n",
    "from langchain.output_parsers.retry import RetryOutputParser\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from typing import List, Optional, Any, Union\n",
    "from langchain.schema import BaseOutputParser\n",
    "\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field, validator\n",
    "\n",
    "from langchain_core.prompts.few_shot import FewShotPromptTemplate\n",
    "from langchain_core.prompts.prompt import PromptTemplate\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import random \n",
    "from time import time \n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "train_data = pd.read_csv(\"data/train_data.csv\")\n",
    "test_data = pd.read_csv(\"data/test_data.csv\")\n",
    "\n",
    "llm = Llama()\n",
    "llm(query=\"test\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c7d4d3-752b-4628-83f6-fb9d366dc17f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2efe4bfa-6cec-4ce0-8d18-4816cbd8c687",
   "metadata": {},
   "source": [
    "## Few-Shot Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09c8cfe0-f6d4-4c25-ad8f-5d5be45e9070",
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------ Helper Methods ------------\n",
    "def get_prompt_length(llm, str_input):\n",
    "    messages = [\n",
    "            {\"role\": \"system\", \"content\": llm._system_msg},\n",
    "            {\"role\": \"user\", \"content\": str_input},\n",
    "        ]\n",
    "    prompt = llm.pipeline.tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True\n",
    "    )\n",
    "    input_tokens_length = len(llm.pipeline.tokenizer.encode(prompt))\n",
    "    prompt_length = len(prompt)\n",
    "    return input_tokens_length,prompt_length\n",
    "def process_row(row):\n",
    "    start = time() \n",
    "    try:\n",
    "        row['prediction'] = main_chain.invoke({\"text\": row['text']})\n",
    "    except ValueError as e:\n",
    "        #print(\"Value Error encountered\")\n",
    "        row['prediction'] = np.nan\n",
    "    end = time()\n",
    "    row['inference_time'] = end-start\n",
    "    input_tokens_length, prompt_length = get_prompt_length(llm, few_shot_prompt.format(text=row['text']))\n",
    "    row['input_tokens_length'] = input_tokens_length\n",
    "    row['prompt_length'] = prompt_length\n",
    "    row['prompt'] = few_shot_prompt.format(text=row['text'])\n",
    "    return row\n",
    "def get_examples(df, num_shots):\n",
    "    examples = []\n",
    "    for label in df.label.unique():\n",
    "        for i, row in df[df['label'] == label].sample(num_shots, random_state=random_seed).iterrows():\n",
    "            examples.append({\n",
    "                \"input\": row['text'],\n",
    "                \"output\": str(label)\n",
    "            })\n",
    "            df = df.drop(i)\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe4ffc03-81fb-42a3-9ceb-aa60b83a73f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21f0d59a12dd4cbca3f3af3223cd7593",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing n_classes:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing num_shots:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d6ae7c67b324e11bd2819374c5d0c3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1600 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    }
   ],
   "source": [
    "#craft examples\n",
    "tqdm.pandas()\n",
    "\n",
    "random.seed(42)\n",
    "random_seed = 42\n",
    "n_classes_list = [40]#,30,50,76]\n",
    "num_shots_list = [1]\n",
    "\n",
    "for n_classes in tqdm(n_classes_list, desc=\"Processing n_classes\"):\n",
    "    for num_shots in tqdm(num_shots_list, desc=\"Processing num_shots\", leave=False):\n",
    "        #sampled_classes = train_data['label'].sample(n_classes, random_state=random_seed, replace=False).values\n",
    "        sampled_classes = random.sample(test_data['label'].unique().tolist(),n_classes)\n",
    "        train_data_sub = train_data[train_data['label'].isin(sampled_classes)]\n",
    "        test_data_sub = test_data[test_data['label'].isin(sampled_classes)]\n",
    "\n",
    "        #print(\"*\"*50)\n",
    "        #print(f\"Number of sampled classes: {len(sampled_classes)}\")\n",
    "        #print(f\"Number of samples in test_data_sub: {len(test_data_sub)}\")\n",
    "\n",
    "\n",
    "        #Achtung: muss hier drin stehen, da sampled_classes verwendet wird. Geht nicht sauberer\n",
    "        class NumberParser(BaseOutputParser):\n",
    "            def parse(self, text: str) -> Any:\n",
    "                try:\n",
    "                    # Extracting the first number from the text\n",
    "                    number = int(text.strip())\n",
    "                    if number not in sampled_classes:\n",
    "                        raise ValueError(f\"Number {number} is not in the list of valid classes.\")\n",
    "                    return number\n",
    "                except ValueError as e:\n",
    "                    raise ValueError(f\"Failed to parse number: {e}\")\n",
    "        \n",
    "\n",
    "        examples = get_examples(test_data_sub, num_shots)\n",
    "        example_prompt = PromptTemplate(\n",
    "            input_variables=[\"input\", \"output\"], template=\"input: {input}\\noutput: {output}\"\n",
    "        )\n",
    "        few_shot_prompt = FewShotPromptTemplate(\n",
    "            prefix=\"Your task is to classify a given text by assigning a label from the examples to it, Please ONLY respond with the label as a number. ONLY use labels you see in the examples, don't make up own labels.\\nHere are some examples:\",\n",
    "            examples=examples,\n",
    "            example_prompt=example_prompt,\n",
    "            suffix=\"input: {text}.\\noutput:\",\n",
    "            input_variables=[\"text\"],\n",
    "        )\n",
    "        completion_chain = few_shot_prompt | llm\n",
    "        \n",
    "        custom_parser = NumberParser()\n",
    "        retry_parser = RetryOutputParser(parser=custom_parser, max_retries=3)\n",
    "        \n",
    "        main_chain = RunnableParallel(\n",
    "            completion=completion_chain, prompt_value=few_shot_prompt\n",
    "        ) | RunnableLambda(lambda x: retry_parser.parse_with_prompt(**x))\n",
    "        \n",
    "        \n",
    "        test_data_sub = test_data_sub.progress_apply(process_row, axis=1)\n",
    "\n",
    "        #-------- Evaluate and log results -----------\n",
    "        file_name = f\"data/few_shot/test/classes_{n_classes}_shots_{num_shots}\"\n",
    "        if not os.path.exists(file_name):\n",
    "            os.makedirs(file_name)\n",
    "        test_data_sub.to_csv(os.path.join(file_name,\"predictions.csv\"),index=False)\n",
    "        nan_count = test_data_sub['prediction'].isna().sum()\n",
    "        test_data_sub['prediction'] = test_data_sub['prediction'].fillna(-1)\n",
    "        #print(f\"Number of NaN values in 'prediction': {nan_count}\")\n",
    "        report = classification_report(test_data_sub['label'], test_data_sub['prediction'], output_dict=True, zero_division=np.nan)\n",
    "        with open(os.path.join(file_name,\"results.json\"), \"w\") as f:\n",
    "            json.dump(report, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "048fc161-fe8e-450b-b05f-ab345c879f64",
   "metadata": {},
   "source": [
    "## Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5d8ba5d-cd9d-41b4-86b6-420a2e24f9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_name = \"data/few_shot/test/classes_40_shots_1\"\n",
    "predictions = pd.read_csv(os.path.join(dir_name,\"predictions.csv\"), index_col=False)\n",
    "with open(os.path.join(dir_name,\"results.json\"), \"r\") as f:\n",
    "        report = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8264b327-63d0-48ce-b6f3-ef0895525904",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>prediction</th>\n",
       "      <th>inference_time</th>\n",
       "      <th>input_tokens_length</th>\n",
       "      <th>prompt_length</th>\n",
       "      <th>prompt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How do I locate my card?</td>\n",
       "      <td>11</td>\n",
       "      <td>41.0</td>\n",
       "      <td>2.139808</td>\n",
       "      <td>905</td>\n",
       "      <td>3540</td>\n",
       "      <td>Your task is to classify a given text by assig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I still have not received my new card, I order...</td>\n",
       "      <td>11</td>\n",
       "      <td>41.0</td>\n",
       "      <td>1.447461</td>\n",
       "      <td>913</td>\n",
       "      <td>3581</td>\n",
       "      <td>Your task is to classify a given text by assig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I ordered a card but it has not arrived. Help ...</td>\n",
       "      <td>11</td>\n",
       "      <td>41.0</td>\n",
       "      <td>1.450712</td>\n",
       "      <td>911</td>\n",
       "      <td>3569</td>\n",
       "      <td>Your task is to classify a given text by assig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Is there a way to know when my card will arrive?</td>\n",
       "      <td>11</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1.453190</td>\n",
       "      <td>910</td>\n",
       "      <td>3564</td>\n",
       "      <td>Your task is to classify a given text by assig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>My card has not arrived yet.</td>\n",
       "      <td>11</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1.455353</td>\n",
       "      <td>904</td>\n",
       "      <td>3544</td>\n",
       "      <td>Your task is to classify a given text by assig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1595</th>\n",
       "      <td>If i'm not in the UK, can I still get a card?</td>\n",
       "      <td>24</td>\n",
       "      <td>24.0</td>\n",
       "      <td>1.496940</td>\n",
       "      <td>913</td>\n",
       "      <td>3561</td>\n",
       "      <td>Your task is to classify a given text by assig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1596</th>\n",
       "      <td>How many countries do you support?</td>\n",
       "      <td>24</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.496049</td>\n",
       "      <td>905</td>\n",
       "      <td>3550</td>\n",
       "      <td>Your task is to classify a given text by assig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1597</th>\n",
       "      <td>What countries do you do business in?</td>\n",
       "      <td>24</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.495343</td>\n",
       "      <td>906</td>\n",
       "      <td>3553</td>\n",
       "      <td>Your task is to classify a given text by assig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1598</th>\n",
       "      <td>What are the countries you operate in.</td>\n",
       "      <td>24</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.497036</td>\n",
       "      <td>905</td>\n",
       "      <td>3554</td>\n",
       "      <td>Your task is to classify a given text by assig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599</th>\n",
       "      <td>Can the card be mailed and used in Europe?</td>\n",
       "      <td>24</td>\n",
       "      <td>33.0</td>\n",
       "      <td>1.493075</td>\n",
       "      <td>908</td>\n",
       "      <td>3558</td>\n",
       "      <td>Your task is to classify a given text by assig...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1600 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  label  prediction  \\\n",
       "0                              How do I locate my card?     11        41.0   \n",
       "1     I still have not received my new card, I order...     11        41.0   \n",
       "2     I ordered a card but it has not arrived. Help ...     11        41.0   \n",
       "3      Is there a way to know when my card will arrive?     11        11.0   \n",
       "4                          My card has not arrived yet.     11        11.0   \n",
       "...                                                 ...    ...         ...   \n",
       "1595      If i'm not in the UK, can I still get a card?     24        24.0   \n",
       "1596                 How many countries do you support?     24         NaN   \n",
       "1597              What countries do you do business in?     24         NaN   \n",
       "1598             What are the countries you operate in.     24         NaN   \n",
       "1599         Can the card be mailed and used in Europe?     24        33.0   \n",
       "\n",
       "      inference_time  input_tokens_length  prompt_length  \\\n",
       "0           2.139808                  905           3540   \n",
       "1           1.447461                  913           3581   \n",
       "2           1.450712                  911           3569   \n",
       "3           1.453190                  910           3564   \n",
       "4           1.455353                  904           3544   \n",
       "...              ...                  ...            ...   \n",
       "1595        1.496940                  913           3561   \n",
       "1596        1.496049                  905           3550   \n",
       "1597        1.495343                  906           3553   \n",
       "1598        1.497036                  905           3554   \n",
       "1599        1.493075                  908           3558   \n",
       "\n",
       "                                                 prompt  \n",
       "0     Your task is to classify a given text by assig...  \n",
       "1     Your task is to classify a given text by assig...  \n",
       "2     Your task is to classify a given text by assig...  \n",
       "3     Your task is to classify a given text by assig...  \n",
       "4     Your task is to classify a given text by assig...  \n",
       "...                                                 ...  \n",
       "1595  Your task is to classify a given text by assig...  \n",
       "1596  Your task is to classify a given text by assig...  \n",
       "1597  Your task is to classify a given text by assig...  \n",
       "1598  Your task is to classify a given text by assig...  \n",
       "1599  Your task is to classify a given text by assig...  \n",
       "\n",
       "[1600 rows x 7 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a018f50b-8504-4794-a9f1-2ea325e0004d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(predictions[\"label\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9fcc9cec-050b-451c-9747-125d29f63966",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'-1.0': {'precision': 0.0, 'recall': nan, 'f1-score': 0.0, 'support': 0.0},\n",
       " '0.0': {'precision': 0.6,\n",
       "  'recall': 0.075,\n",
       "  'f1-score': 0.13333333333333333,\n",
       "  'support': 40.0},\n",
       " '1.0': {'precision': 1.0,\n",
       "  'recall': 0.8,\n",
       "  'f1-score': 0.8888888888888888,\n",
       "  'support': 40.0},\n",
       " '2.0': {'precision': 1.0,\n",
       "  'recall': 0.025,\n",
       "  'f1-score': 0.04878048780487805,\n",
       "  'support': 40.0},\n",
       " '5.0': {'precision': 1.0,\n",
       "  'recall': 0.025,\n",
       "  'f1-score': 0.04878048780487805,\n",
       "  'support': 40.0},\n",
       " '6.0': {'precision': 1.0,\n",
       "  'recall': 0.05,\n",
       "  'f1-score': 0.09523809523809523,\n",
       "  'support': 40.0},\n",
       " '7.0': {'precision': 1.0,\n",
       "  'recall': 0.025,\n",
       "  'f1-score': 0.04878048780487805,\n",
       "  'support': 40.0},\n",
       " '8.0': {'precision': 1.0,\n",
       "  'recall': 0.1,\n",
       "  'f1-score': 0.18181818181818182,\n",
       "  'support': 40.0},\n",
       " '11.0': {'precision': 1.0,\n",
       "  'recall': 0.375,\n",
       "  'f1-score': 0.5454545454545454,\n",
       "  'support': 40.0},\n",
       " '13.0': {'precision': 0.6521739130434783,\n",
       "  'recall': 0.375,\n",
       "  'f1-score': 0.47619047619047616,\n",
       "  'support': 40.0},\n",
       " '14.0': {'precision': 0.15028901734104047,\n",
       "  'recall': 0.65,\n",
       "  'f1-score': 0.24413145539906103,\n",
       "  'support': 40.0},\n",
       " '16.0': {'precision': 0.23809523809523808,\n",
       "  'recall': 0.125,\n",
       "  'f1-score': 0.16393442622950818,\n",
       "  'support': 40.0},\n",
       " '17.0': {'precision': 0.8604651162790697,\n",
       "  'recall': 0.925,\n",
       "  'f1-score': 0.891566265060241,\n",
       "  'support': 40.0},\n",
       " '18.0': {'precision': nan, 'recall': 0.0, 'f1-score': 0.0, 'support': 40.0},\n",
       " '19.0': {'precision': 0.07142857142857142,\n",
       "  'recall': 0.025,\n",
       "  'f1-score': 0.037037037037037035,\n",
       "  'support': 40.0},\n",
       " '20.0': {'precision': nan, 'recall': 0.0, 'f1-score': 0.0, 'support': 40.0},\n",
       " '22.0': {'precision': nan, 'recall': 0.0, 'f1-score': 0.0, 'support': 40.0},\n",
       " '23.0': {'precision': 1.0, 'recall': 0.25, 'f1-score': 0.4, 'support': 40.0},\n",
       " '24.0': {'precision': 0.4666666666666667,\n",
       "  'recall': 0.525,\n",
       "  'f1-score': 0.49411764705882355,\n",
       "  'support': 40.0},\n",
       " '25.0': {'precision': 1.0,\n",
       "  'recall': 0.05,\n",
       "  'f1-score': 0.09523809523809523,\n",
       "  'support': 40.0},\n",
       " '27.0': {'precision': 1.0,\n",
       "  'recall': 0.025,\n",
       "  'f1-score': 0.04878048780487805,\n",
       "  'support': 40.0},\n",
       " '28.0': {'precision': 1.0,\n",
       "  'recall': 0.05,\n",
       "  'f1-score': 0.09523809523809523,\n",
       "  'support': 40.0},\n",
       " '29.0': {'precision': 0.9655172413793104,\n",
       "  'recall': 0.7,\n",
       "  'f1-score': 0.8115942028985508,\n",
       "  'support': 40.0},\n",
       " '30.0': {'precision': 0.8,\n",
       "  'recall': 0.6,\n",
       "  'f1-score': 0.6857142857142857,\n",
       "  'support': 40.0},\n",
       " '31.0': {'precision': 1.0,\n",
       "  'recall': 0.025,\n",
       "  'f1-score': 0.04878048780487805,\n",
       "  'support': 40.0},\n",
       " '33.0': {'precision': 0.38,\n",
       "  'recall': 0.475,\n",
       "  'f1-score': 0.4222222222222222,\n",
       "  'support': 40.0},\n",
       " '34.0': {'precision': 0.25663716814159293,\n",
       "  'recall': 0.725,\n",
       "  'f1-score': 0.3790849673202614,\n",
       "  'support': 40.0},\n",
       " '36.0': {'precision': 0.2761904761904762,\n",
       "  'recall': 0.725,\n",
       "  'f1-score': 0.4,\n",
       "  'support': 40.0},\n",
       " '38.0': {'precision': 0.625,\n",
       "  'recall': 0.25,\n",
       "  'f1-score': 0.35714285714285715,\n",
       "  'support': 40.0},\n",
       " '41.0': {'precision': 0.1291390728476821,\n",
       "  'recall': 0.975,\n",
       "  'f1-score': 0.22807017543859648,\n",
       "  'support': 40.0},\n",
       " '42.0': {'precision': 1.0,\n",
       "  'recall': 0.025,\n",
       "  'f1-score': 0.04878048780487805,\n",
       "  'support': 40.0},\n",
       " '43.0': {'precision': 0.25,\n",
       "  'recall': 0.05,\n",
       "  'f1-score': 0.08333333333333333,\n",
       "  'support': 40.0},\n",
       " '44.0': {'precision': 1.0,\n",
       "  'recall': 0.775,\n",
       "  'f1-score': 0.8732394366197183,\n",
       "  'support': 40.0},\n",
       " '46.0': {'precision': 0.16230366492146597,\n",
       "  'recall': 0.775,\n",
       "  'f1-score': 0.2683982683982684,\n",
       "  'support': 40.0},\n",
       " '49.0': {'precision': 0.5405405405405406,\n",
       "  'recall': 0.5,\n",
       "  'f1-score': 0.5194805194805194,\n",
       "  'support': 40.0},\n",
       " '51.0': {'precision': 0.7142857142857143,\n",
       "  'recall': 0.125,\n",
       "  'f1-score': 0.2127659574468085,\n",
       "  'support': 40.0},\n",
       " '57.0': {'precision': 0.7692307692307693,\n",
       "  'recall': 0.25,\n",
       "  'f1-score': 0.37735849056603776,\n",
       "  'support': 40.0},\n",
       " '66.0': {'precision': 0.875,\n",
       "  'recall': 0.175,\n",
       "  'f1-score': 0.2916666666666667,\n",
       "  'support': 40.0},\n",
       " '69.0': {'precision': 0.3490566037735849,\n",
       "  'recall': 0.925,\n",
       "  'f1-score': 0.5068493150684932,\n",
       "  'support': 40.0},\n",
       " '72.0': {'precision': 1.0,\n",
       "  'recall': 0.05,\n",
       "  'f1-score': 0.09523809523809523,\n",
       "  'support': 40.0},\n",
       " '73.0': {'precision': 0.5,\n",
       "  'recall': 0.525,\n",
       "  'f1-score': 0.5121951219512195,\n",
       "  'support': 40.0},\n",
       " 'accuracy': 0.328125,\n",
       " 'macro avg': {'precision': 0.6745268361622422,\n",
       "  'recall': 0.328125,\n",
       "  'f1-score': 0.29412739962242884,\n",
       "  'support': 1600.0},\n",
       " 'weighted avg': {'precision': 0.692757291193654,\n",
       "  'recall': 0.328125,\n",
       "  'f1-score': 0.3014805846129896,\n",
       "  'support': 1600.0}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2caddd5-38dd-4490-beef-9056ff53bb86",
   "metadata": {},
   "source": [
    "## Similarity Enhanced Few-Shot Prompting\n",
    "\n",
    "Idee: wähle few shot examples basierend auf der similarity zur query aus -> reduziere damit kontextlänge.\n",
    "- FAISS vector storage\n",
    "- HuggingFace Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1f56cf29-70bd-4fff-a6cb-db6dbdfba1cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/pfs/data5/home/kit/anthropomatik/yy5819/llm_classification/venv/lib64/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "\n",
    "model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "model_kwargs = {'device': 'cuda'}\n",
    "encode_kwargs = {'normalize_embeddings': False}\n",
    "hf = HuggingFaceEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")\n",
    "from langchain_core.example_selectors import SemanticSimilarityExampleSelector\n",
    "\n",
    "example_selector = SemanticSimilarityExampleSelector.from_examples(\n",
    "    examples,\n",
    "    hf,\n",
    "    FAISS,\n",
    "    k=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b4f66a86-75d6-405e-aacd-fd4cf703e1e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71f8d06a1a1248c78ee8a014a6597cf4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing n_classes:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bef8367d5f9249db83a425ecaf42582b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing num_shots:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f40e537fab2049cca76ed194df899a79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1720 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are a helpful assistant<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Your task is to classify a given text by assigning a label from the examples to it, Please ONLY respond with the label as a number. ONLY use labels you see in the examples, don't make up own labels.\n",
      "Here are some examples:\n",
      "\n",
      "input: I would like to change my pin.\n",
      "output: 21\n",
      "\n",
      "input: My bank transfer is still not showing up in my account.\n",
      "output: 5\n",
      "\n",
      "input: How do I locate my card?.\n",
      "output:<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are a helpful assistant<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Your task is to classify a given text by assigning a label from the examples to it, Please ONLY respond with the label as a number. ONLY use labels you see in the examples, don't make up own labels.\n",
      "Here are some examples:\n",
      "\n",
      "input: My bank transfer is still not showing up in my account.\n",
      "output: 5\n",
      "\n",
      "input: I topped up but the app did not accept it.\n",
      "output: 59\n",
      "\n",
      "input: I still have not received my new card, I ordered over a week ago..\n",
      "output:<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are a helpful assistant<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Your task is to classify a given text by assigning a label from the examples to it, Please ONLY respond with the label as a number. ONLY use labels you see in the examples, don't make up own labels.\n",
      "Here are some examples:\n",
      "\n",
      "input: My bank transfer is still not showing up in my account.\n",
      "output: 5\n",
      "\n",
      "input: Why would I be charged a fee for card payment?\n",
      "output: 15\n",
      "\n",
      "input: I ordered a card but it has not arrived. Help please!.\n",
      "output:<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are a helpful assistant<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Your task is to classify a given text by assigning a label from the examples to it, Please ONLY respond with the label as a number. ONLY use labels you see in the examples, don't make up own labels.\n",
      "Here are some examples:\n",
      "\n",
      "input: My bank transfer is still not showing up in my account.\n",
      "output: 5\n",
      "\n",
      "input: Why would I be charged a fee for card payment?\n",
      "output: 15\n",
      "\n",
      "input: Is there a way to know when my card will arrive?.\n",
      "output:<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are a helpful assistant<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Your task is to classify a given text by assigning a label from the examples to it, Please ONLY respond with the label as a number. ONLY use labels you see in the examples, don't make up own labels.\n",
      "Here are some examples:\n",
      "\n",
      "input: My bank transfer is still not showing up in my account.\n",
      "output: 5\n",
      "\n",
      "input: I topped up but the app did not accept it.\n",
      "output: 59\n",
      "\n",
      "input: My card has not arrived yet..\n",
      "output:<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are a helpful assistant<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Your task is to classify a given text by assigning a label from the examples to it, Please ONLY respond with the label as a number. ONLY use labels you see in the examples, don't make up own labels.\n",
      "Here are some examples:\n",
      "\n",
      "input: Why would I be charged a fee for card payment?\n",
      "output: 15\n",
      "\n",
      "input: My bank transfer is still not showing up in my account.\n",
      "output: 5\n",
      "\n",
      "input: When will I get my card?.\n",
      "output:<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[62], line 52\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m#tqdm.pandas()\u001b[39;00m\n\u001b[1;32m     47\u001b[0m main_chain \u001b[38;5;241m=\u001b[39m RunnableParallel(\n\u001b[1;32m     48\u001b[0m     completion\u001b[38;5;241m=\u001b[39mcompletion_chain, prompt_value\u001b[38;5;241m=\u001b[39mfew_shot_prompt\n\u001b[1;32m     49\u001b[0m ) \u001b[38;5;241m|\u001b[39m RunnableLambda(\u001b[38;5;28;01mlambda\u001b[39;00m x: retry_parser\u001b[38;5;241m.\u001b[39mparse_with_prompt(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mx))\n\u001b[0;32m---> 52\u001b[0m test_data_sub \u001b[38;5;241m=\u001b[39m \u001b[43mtest_data_sub\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprogress_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_row\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m#-------- Evaluate and log results -----------\u001b[39;00m\n\u001b[1;32m     55\u001b[0m file_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/enhanced_few_shot/test/classes_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_classes\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_shots_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_shots\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m/pfs/data5/home/kit/anthropomatik/yy5819/llm_classification/venv/lib64/python3.9/site-packages/tqdm/std.py:917\u001b[0m, in \u001b[0;36mtqdm.pandas.<locals>.inner_generator.<locals>.inner\u001b[0;34m(df, func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    914\u001b[0m \u001b[38;5;66;03m# Apply the provided function (in **kwargs)\u001b[39;00m\n\u001b[1;32m    915\u001b[0m \u001b[38;5;66;03m# on the df using our wrapper (which provides bar updating)\u001b[39;00m\n\u001b[1;32m    916\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 917\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf_function\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwrapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    918\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    919\u001b[0m     t\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m/pfs/data5/home/kit/anthropomatik/yy5819/llm_classification/venv/lib64/python3.9/site-packages/pandas/core/frame.py:10374\u001b[0m, in \u001b[0;36mDataFrame.apply\u001b[0;34m(self, func, axis, raw, result_type, args, by_row, engine, engine_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m  10360\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapply\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m frame_apply\n\u001b[1;32m  10362\u001b[0m op \u001b[38;5;241m=\u001b[39m frame_apply(\n\u001b[1;32m  10363\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m  10364\u001b[0m     func\u001b[38;5;241m=\u001b[39mfunc,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m  10372\u001b[0m     kwargs\u001b[38;5;241m=\u001b[39mkwargs,\n\u001b[1;32m  10373\u001b[0m )\n\u001b[0;32m> 10374\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapply\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/pfs/data5/home/kit/anthropomatik/yy5819/llm_classification/venv/lib64/python3.9/site-packages/pandas/core/apply.py:916\u001b[0m, in \u001b[0;36mFrameApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw:\n\u001b[1;32m    914\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_raw(engine\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine, engine_kwargs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine_kwargs)\n\u001b[0;32m--> 916\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/pfs/data5/home/kit/anthropomatik/yy5819/llm_classification/venv/lib64/python3.9/site-packages/pandas/core/apply.py:1063\u001b[0m, in \u001b[0;36mFrameApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1061\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_standard\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m   1062\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpython\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 1063\u001b[0m         results, res_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_series_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1064\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1065\u001b[0m         results, res_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_series_numba()\n",
      "File \u001b[0;32m/pfs/data5/home/kit/anthropomatik/yy5819/llm_classification/venv/lib64/python3.9/site-packages/pandas/core/apply.py:1081\u001b[0m, in \u001b[0;36mFrameApply.apply_series_generator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1078\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m option_context(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode.chained_assignment\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   1079\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(series_gen):\n\u001b[1;32m   1080\u001b[0m         \u001b[38;5;66;03m# ignore SettingWithCopy here in case the user mutates\u001b[39;00m\n\u001b[0;32m-> 1081\u001b[0m         results[i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1082\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(results[i], ABCSeries):\n\u001b[1;32m   1083\u001b[0m             \u001b[38;5;66;03m# If we have a view on v, we need to make a copy because\u001b[39;00m\n\u001b[1;32m   1084\u001b[0m             \u001b[38;5;66;03m#  series_generator will swap out the underlying data\u001b[39;00m\n\u001b[1;32m   1085\u001b[0m             results[i] \u001b[38;5;241m=\u001b[39m results[i]\u001b[38;5;241m.\u001b[39mcopy(deep\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m/pfs/data5/home/kit/anthropomatik/yy5819/llm_classification/venv/lib64/python3.9/site-packages/tqdm/std.py:912\u001b[0m, in \u001b[0;36mtqdm.pandas.<locals>.inner_generator.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    906\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    907\u001b[0m     \u001b[38;5;66;03m# update tbar correctly\u001b[39;00m\n\u001b[1;32m    908\u001b[0m     \u001b[38;5;66;03m# it seems `pandas apply` calls `func` twice\u001b[39;00m\n\u001b[1;32m    909\u001b[0m     \u001b[38;5;66;03m# on the first column/row to decide whether it can\u001b[39;00m\n\u001b[1;32m    910\u001b[0m     \u001b[38;5;66;03m# take a fast or slow code path; so stop when t.total==t.n\u001b[39;00m\n\u001b[1;32m    911\u001b[0m     t\u001b[38;5;241m.\u001b[39mupdate(n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m t\u001b[38;5;241m.\u001b[39mtotal \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mn \u001b[38;5;241m<\u001b[39m t\u001b[38;5;241m.\u001b[39mtotal \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m--> 912\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[58], line 19\u001b[0m, in \u001b[0;36mprocess_row\u001b[0;34m(row)\u001b[0m\n\u001b[1;32m     17\u001b[0m start \u001b[38;5;241m=\u001b[39m time() \n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 19\u001b[0m     row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprediction\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mmain_chain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;66;03m#print(\"Value Error encountered\")\u001b[39;00m\n\u001b[1;32m     22\u001b[0m     row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprediction\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mnan\n",
      "File \u001b[0;32m/pfs/data5/home/kit/anthropomatik/yy5819/llm_classification/venv/lib64/python3.9/site-packages/langchain_core/runnables/base.py:2505\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   2501\u001b[0m config \u001b[38;5;241m=\u001b[39m patch_config(\n\u001b[1;32m   2502\u001b[0m     config, callbacks\u001b[38;5;241m=\u001b[39mrun_manager\u001b[38;5;241m.\u001b[39mget_child(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseq:step:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2503\u001b[0m )\n\u001b[1;32m   2504\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2505\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2506\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2507\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m step\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "File \u001b[0;32m/pfs/data5/home/kit/anthropomatik/yy5819/llm_classification/venv/lib64/python3.9/site-packages/langchain_core/runnables/base.py:3152\u001b[0m, in \u001b[0;36mRunnableParallel.invoke\u001b[0;34m(self, input, config)\u001b[0m\n\u001b[1;32m   3139\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m get_executor_for_config(config) \u001b[38;5;28;01mas\u001b[39;00m executor:\n\u001b[1;32m   3140\u001b[0m         futures \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m   3141\u001b[0m             executor\u001b[38;5;241m.\u001b[39msubmit(\n\u001b[1;32m   3142\u001b[0m                 step\u001b[38;5;241m.\u001b[39minvoke,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3150\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m key, step \u001b[38;5;129;01min\u001b[39;00m steps\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m   3151\u001b[0m         ]\n\u001b[0;32m-> 3152\u001b[0m         output \u001b[38;5;241m=\u001b[39m {key: future\u001b[38;5;241m.\u001b[39mresult() \u001b[38;5;28;01mfor\u001b[39;00m key, future \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(steps, futures)}\n\u001b[1;32m   3153\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[1;32m   3154\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/pfs/data5/home/kit/anthropomatik/yy5819/llm_classification/venv/lib64/python3.9/site-packages/langchain_core/runnables/base.py:3152\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   3139\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m get_executor_for_config(config) \u001b[38;5;28;01mas\u001b[39;00m executor:\n\u001b[1;32m   3140\u001b[0m         futures \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m   3141\u001b[0m             executor\u001b[38;5;241m.\u001b[39msubmit(\n\u001b[1;32m   3142\u001b[0m                 step\u001b[38;5;241m.\u001b[39minvoke,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3150\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m key, step \u001b[38;5;129;01min\u001b[39;00m steps\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m   3151\u001b[0m         ]\n\u001b[0;32m-> 3152\u001b[0m         output \u001b[38;5;241m=\u001b[39m {key: \u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m key, future \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(steps, futures)}\n\u001b[1;32m   3153\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[1;32m   3154\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/usr/lib64/python3.9/concurrent/futures/_base.py:441\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[1;32m    439\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__get_result()\n\u001b[0;32m--> 441\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_condition\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    443\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\u001b[1;32m    444\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n",
      "File \u001b[0;32m/usr/lib64/python3.9/threading.py:312\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    311\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 312\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    313\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    314\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "random_seed = 42\n",
    "n_classes_list = [5,10]\n",
    "num_shots_list = [1]\n",
    "\n",
    "for n_classes in tqdm(n_classes_list, desc=\"Processing n_classes\"):\n",
    "    for num_shots in tqdm(num_shots_list, desc=\"Processing num_shots\", leave=False):\n",
    "        #sampled_classes = train_data['label'].sample(n_classes, random_state=random_seed).values\n",
    "        sampled_classes = random.sample(test_data['label'].unique().tolist(),n_classes)\n",
    "        train_data_sub = train_data[train_data['label'].isin(sampled_classes)]\n",
    "        test_data_sub = test_data[test_data['label'].isin(sampled_classes)]\n",
    "\n",
    "        #print(\"*\"*50)\n",
    "        #print(f\"Number of sampled classes: {len(sampled_classes)}\")\n",
    "        #print(f\"Number of samples in test_data_sub: {len(test_data_sub)}\")\n",
    "\n",
    "\n",
    "        #Achtung: muss hier drin stehen, da sampled_classes verwendet wird. Geht nicht sauberer\n",
    "        class NumberParser(BaseOutputParser):\n",
    "            def parse(self, text: str) -> Any:\n",
    "                try:\n",
    "                    # Extracting the first number from the text\n",
    "                    number = int(text.strip())\n",
    "                    if number not in sampled_classes:\n",
    "                        raise ValueError(f\"Number {number} is not in the list of valid classes.\")\n",
    "                    return number\n",
    "                except ValueError as e:\n",
    "                    raise ValueError(f\"Failed to parse number: {e}\")\n",
    "        \n",
    "\n",
    "        examples = get_examples(test_data_sub, num_shots)\n",
    "        example_prompt = PromptTemplate(\n",
    "            input_variables=[\"input\", \"output\"], template=\"input: {input}\\noutput: {output}\"\n",
    "        )\n",
    "        few_shot_prompt = FewShotPromptTemplate(\n",
    "            prefix=\"Your task is to classify a given text by assigning a label from the examples to it, Please ONLY respond with the label as a number. ONLY use labels you see in the examples, don't make up own labels.\\nHere are some examples:\",\n",
    "            example_selector=example_selector,\n",
    "            example_prompt=example_prompt,\n",
    "            suffix=\"input: {text}.\\noutput:\",\n",
    "            input_variables=[\"text\"],\n",
    "        )\n",
    "        completion_chain = few_shot_prompt | llm\n",
    "        \n",
    "        custom_parser = NumberParser()\n",
    "        retry_parser = RetryOutputParser(parser=custom_parser, max_retries=3)\n",
    "\n",
    "        #tqdm.pandas()\n",
    "        \n",
    "        main_chain = RunnableParallel(\n",
    "            completion=completion_chain, prompt_value=few_shot_prompt\n",
    "        ) | RunnableLambda(lambda x: retry_parser.parse_with_prompt(**x))\n",
    "        \n",
    "        \n",
    "        test_data_sub = test_data_sub.progress_apply(process_row, axis=1)\n",
    "\n",
    "        #-------- Evaluate and log results -----------\n",
    "        file_name = f\"data/enhanced_few_shot/test/classes_{n_classes}_shots_{num_shots}\"\n",
    "        if not os.path.exists(file_name):\n",
    "            os.makedirs(file_name)\n",
    "        test_data_sub.to_csv(os.path.join(file_name,\"predictions.csv\"), index=False)\n",
    "        nan_count = test_data_sub['prediction'].isna().sum()\n",
    "        test_data_sub['prediction'] = test_data_sub['prediction'].fillna(-1)\n",
    "        #print(f\"Number of NaN values in 'prediction': {nan_count}\")\n",
    "        report = classification_report(test_data_sub['label'], test_data_sub['prediction'], output_dict=True, zero_division=np.nan)\n",
    "        with open(os.path.join(file_name,\"results.json\"), \"w\") as f:\n",
    "            json.dump(report, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8d92097d-2885-47ef-b63d-0684f2d1ffa2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FewShotPromptTemplate(input_variables=['text'], example_selector=SemanticSimilarityExampleSelector(vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x149480ea7520>, k=2, example_keys=None, input_keys=None, vectorstore_kwargs=None), example_prompt=PromptTemplate(input_variables=['input', 'output'], template='input: {input}\\noutput: {output}'), suffix='input: {text}.\\noutput:', prefix=\"Your task is to classify a given text by assigning a label from the examples to it, Please ONLY respond with the label as a number. ONLY use labels you see in the examples, don't make up own labels.\\nHere are some examples:\")"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "few_shot_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "16faf77f-0d24-41c6-b312-1086f00b2fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pd.read_csv(os.path.join(file_name,\"predictions.csv\"), index_col=False)\n",
    "with open(os.path.join(file_name,\"results.json\"), \"r\") as f:\n",
    "        report = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "351fa7ba-b963-4583-87a4-0ddfe6d56f9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'-1.0': {'precision': 0.0, 'recall': nan, 'f1-score': 0.0, 'support': 0.0},\n",
       " '5.0': {'precision': 0.875,\n",
       "  'recall': 0.875,\n",
       "  'f1-score': 0.875,\n",
       "  'support': 40.0},\n",
       " '15.0': {'precision': 0.7407407407407407,\n",
       "  'recall': 1.0,\n",
       "  'f1-score': 0.851063829787234,\n",
       "  'support': 40.0},\n",
       " '21.0': {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 40.0},\n",
       " '25.0': {'precision': 0.6774193548387096,\n",
       "  'recall': 0.525,\n",
       "  'f1-score': 0.5915492957746479,\n",
       "  'support': 40.0},\n",
       " '59.0': {'precision': 0.9090909090909091,\n",
       "  'recall': 0.75,\n",
       "  'f1-score': 0.821917808219178,\n",
       "  'support': 40.0},\n",
       " 'accuracy': 0.83,\n",
       " 'macro avg': {'precision': 0.7003751674450598,\n",
       "  'recall': 0.8300000000000001,\n",
       "  'f1-score': 0.6899218222968434,\n",
       "  'support': 200.0},\n",
       " 'weighted avg': {'precision': 0.8404502009340717,\n",
       "  'recall': 0.83,\n",
       "  'f1-score': 0.8279061867562121,\n",
       "  'support': 200.0}}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2480ee3c-0ad2-47a6-90f4-c1192e2723d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>prediction</th>\n",
       "      <th>inference_time</th>\n",
       "      <th>input_tokens_length</th>\n",
       "      <th>prompt_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I payed with a card and was charged an extra fee</td>\n",
       "      <td>15</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.101417</td>\n",
       "      <td>123</td>\n",
       "      <td>625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I was charged a fee after using my card and I ...</td>\n",
       "      <td>15</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.067487</td>\n",
       "      <td>126</td>\n",
       "      <td>631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Is there a reason I was charged a fee for usin...</td>\n",
       "      <td>15</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.277539</td>\n",
       "      <td>135</td>\n",
       "      <td>680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I show another charge on my card from when I u...</td>\n",
       "      <td>15</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.066541</td>\n",
       "      <td>127</td>\n",
       "      <td>635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Why do I get charged additional fees on some p...</td>\n",
       "      <td>15</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.069096</td>\n",
       "      <td>126</td>\n",
       "      <td>647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>Where is my money? I transfered it and it isn'...</td>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.067518</td>\n",
       "      <td>128</td>\n",
       "      <td>635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>I need to know the time frame that is typical ...</td>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.279467</td>\n",
       "      <td>149</td>\n",
       "      <td>750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>Why doesn't my balance reflect my transfer</td>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.069087</td>\n",
       "      <td>120</td>\n",
       "      <td>615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>My account balance has not gone up even though...</td>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.071471</td>\n",
       "      <td>127</td>\n",
       "      <td>652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>When will I see my new balance after making my...</td>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.068249</td>\n",
       "      <td>125</td>\n",
       "      <td>634</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  label  prediction  \\\n",
       "0     I payed with a card and was charged an extra fee     15        15.0   \n",
       "1    I was charged a fee after using my card and I ...     15        15.0   \n",
       "2    Is there a reason I was charged a fee for usin...     15        15.0   \n",
       "3    I show another charge on my card from when I u...     15        15.0   \n",
       "4    Why do I get charged additional fees on some p...     15        15.0   \n",
       "..                                                 ...    ...         ...   \n",
       "195  Where is my money? I transfered it and it isn'...      5         5.0   \n",
       "196  I need to know the time frame that is typical ...      5         5.0   \n",
       "197         Why doesn't my balance reflect my transfer      5         5.0   \n",
       "198  My account balance has not gone up even though...      5         5.0   \n",
       "199  When will I see my new balance after making my...      5         5.0   \n",
       "\n",
       "     inference_time  input_tokens_length  prompt_length  \n",
       "0          1.101417                  123            625  \n",
       "1          1.067487                  126            631  \n",
       "2          1.277539                  135            680  \n",
       "3          1.066541                  127            635  \n",
       "4          1.069096                  126            647  \n",
       "..              ...                  ...            ...  \n",
       "195        1.067518                  128            635  \n",
       "196        1.279467                  149            750  \n",
       "197        1.069087                  120            615  \n",
       "198        1.071471                  127            652  \n",
       "199        1.068249                  125            634  \n",
       "\n",
       "[200 rows x 6 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b275f7c4-ef00-44d7-bc21-29a7f4d32948",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80fdcec6-4b0d-48b2-aede-d846d03ca47c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465e41b7-162c-40f3-b675-2d2ebebd03d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d2a68e-d8d8-4da3-b388-afd41824c879",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c9e3a76f-29fd-4ea6-ab39-005050dbc598",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples most similar to the input: Why does my credit card not work?\n",
      "\n",
      "\n",
      "text: Can you tell me why my card keeps getting declined every time I try to use it?\n",
      "label: 25\n",
      "\n",
      "\n",
      "text: Why do you keep declining my payment? I tried several times already with this card and it is just not working.\n",
      "label: 25\n"
     ]
    }
   ],
   "source": [
    "# TEST the example selector\n",
    "\n",
    "# Select the most similar example to the input.\n",
    "text = \"Why does my credit card not work?\"\n",
    "selected_examples = example_selector.select_examples({\"text\": text})\n",
    "print(f\"Examples most similar to the input: {text}\")\n",
    "for example in selected_examples:\n",
    "    print(\"\\n\")\n",
    "    for k, v in example.items():\n",
    "        print(f\"{k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a8042963-ee20-4a0c-a494-9b18f922d3de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: Can you tell me why my card keeps getting declined every time I try to use it?\n",
      "Label: 25\n",
      "\n",
      "Text: Why do you keep declining my payment? I tried several times already with this card and it is just not working.\n",
      "Label: 25\n",
      "\n",
      "Text: Why does my credit card not work?\n"
     ]
    }
   ],
   "source": [
    "enhanced_few_shot_prompt = FewShotPromptTemplate(\n",
    "    example_selector=example_selector,\n",
    "    example_prompt=example_prompt,\n",
    "    suffix=\"Text: {text}\",\n",
    "    input_variables=[\"text\"],\n",
    ")\n",
    "\n",
    "print(enhanced_few_shot_prompt.format(text=text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3589657-237a-4157-9d41-7e759e840e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 42\n",
    "n_classes_list = [1]\n",
    "num_shots_list = [1]\n",
    "\n",
    "for n_classes in tqdm(n_classes_list, desc=\"Processing n_classes\"):\n",
    "    for num_shots in tqdm(num_shots_list, desc=\"Processing num_shots\", leave=False):\n",
    "        sampled_classes = train_data['label'].sample(n_classes, random_state=random_seed).values\n",
    "        train_data_sub = train_data[train_data['label'].isin(sampled_classes)]\n",
    "        test_data_sub = test_data[test_data['label'].isin(sampled_classes)]\n",
    "\n",
    "        #print(\"*\"*50)\n",
    "        #print(f\"Number of sampled classes: {len(sampled_classes)}\")\n",
    "        #print(f\"Number of samples in test_data_sub: {len(test_data_sub)}\")\n",
    "\n",
    "\n",
    "        #Achtung: muss hier drin stehen, da sampled_classes verwendet wird. Geht nicht sauberer\n",
    "        class NumberParser(BaseOutputParser):\n",
    "            def parse(self, text: str) -> Any:\n",
    "                try:\n",
    "                    # Extracting the first number from the text\n",
    "                    number = int(text.strip())\n",
    "                    if number not in sampled_classes:\n",
    "                        raise ValueError(f\"Number {number} is not in the list of valid classes.\")\n",
    "                    return number\n",
    "                except ValueError as e:\n",
    "                    raise ValueError(f\"Failed to parse number: {e}\")\n",
    "        \n",
    "\n",
    "        examples = get_examples(test_data_sub, num_shots)\n",
    "        example_prompt = PromptTemplate(\n",
    "            input_variables=[\"input\", \"output\"], template=\"input: {input}\\noutput: {output}\"\n",
    "        )\n",
    "        few_shot_prompt = FewShotPromptTemplate(\n",
    "            prefix=\"Your task is to classify a given text by assigning a label from the examples to it, Please ONLY respond with the label as a number. ONLY use labels you see in the examples, don't make up own labels.\\nHere are some examples:\",\n",
    "            examples=examples,\n",
    "            example_selector=example_selector,\n",
    "            example_prompt=example_prompt,\n",
    "            suffix=\"input: {text}.\\noutput:\",\n",
    "            input_variables=[\"text\"],\n",
    "        )\n",
    "        completion_chain = few_shot_prompt | llm\n",
    "        \n",
    "        custom_parser = NumberParser()\n",
    "        retry_parser = RetryOutputParser(parser=custom_parser, max_retries=3)\n",
    "        \n",
    "        main_chain = RunnableParallel(\n",
    "            completion=completion_chain, prompt_value=few_shot_prompt\n",
    "        ) | RunnableLambda(lambda x: retry_parser.parse_with_prompt(**x))\n",
    "        \n",
    "        \n",
    "        test_data_sub = test_data_sub.apply(process_row, axis=1)\n",
    "\n",
    "        #-------- Evaluate and log results -----------\n",
    "        file_name = f\"data/enhanced_few_shot/test/classes_{n_classes}_shots_{num_shots}\"\n",
    "        if not os.path.exists(file_name):\n",
    "            os.makedirs(file_name)\n",
    "        test_data_sub.to_csv(os.path.join(file_name,\"predictions.csv\"))\n",
    "        nan_count = test_data_sub['prediction'].isna().sum()\n",
    "        test_data_sub['prediction'] = test_data_sub['prediction'].fillna(0)\n",
    "        #print(f\"Number of NaN values in 'prediction': {nan_count}\")\n",
    "        report = classification_report(test_data_sub['label'], test_data_sub['prediction'], output_dict=True, zero_division=np.nan)\n",
    "        with open(os.path.join(file_name,\"results.json\"), \"w\") as f:\n",
    "            json.dump(report, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7f65567b-7bb7-4fbf-8857-5bf0c22ce967",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86\n",
      "460\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"I'd be happy to help you troubleshoot the issue with your credit card!\\n\\nThere could be several reasons why your card is being declined. Here are a few common causes:\\n\\n1. **Insufficient funds**: Make sure you have enough available credit on your card to cover the transaction.\\n2. **Expired card**: Check the expiration date on your card to ensure it's still valid.\\n3. **Invalid card information**: Double-check that you're entering the correct card number, expiration date, and security code.\\n4. **Card issuer restrictions**: Your card issuer might have placed a hold on your account or restricted certain types of transactions.\\n5. **Card security features**: Some cards have security features that can cause transactions to be declined, such as a chip or a digital signature.\\n6. **Merchant-specific issues**: The merchant's payment processing system might be experiencing technical difficulties or have specific requirements for card acceptance.\\n\\nTo resolve the issue, you can try the following:\\n\\n1. Contact your card issuer's customer support to report the issue and ask for assistance.\\n2. Check your account balance and available credit to ensure you have sufficient funds.\\n3. Try using a different payment method, such as a different credit card or a debit card.\\n4. Contact the merchant's customer support to\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = llm(query=enhanced_few_shot_prompt.format(text=text))\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "06c78986-86a3-4216-83a9-43d49174433a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompt_length(llm, str_input):\n",
    "    messages = [\n",
    "            {\"role\": \"system\", \"content\": llm._system_msg},\n",
    "            {\"role\": \"user\", \"content\": str_input},\n",
    "        ]\n",
    "    prompt = llm.pipeline.tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True\n",
    "    )\n",
    "    input_tokens_length = len(llm.pipeline.tokenizer.encode(prompt))\n",
    "    prompt_length = len(prompt)\n",
    "    return input_tokens_length,prompt_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8c3a4004-af12-4739-8dd4-f8cd33a539a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tokens_length, prompt_length = get_prompt_length(llm, enhanced_few_shot_prompt.format(text=text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "41624151-4f5a-4595-850e-493158493a15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "86"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tokens_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d41839bb-dcdc-4ead-8efa-e5f300a95799",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "460"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5b98c48b-b924-4245-acbe-0d6cb7735df3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Text: Why do you keep declining my payment? I tried several times already with this card and it is just not working.\\nLabel: 25\\n\\nText: What is the reason my payment was not accepted?\\nLabel: 25\\n\\nText: Your task is to classify a given text into one of the following classes, reply ONLY with the class label: \\n\\nLabel: 25\\nText: Why are you declining my payment? Everything was fine.\\nText: I have a card payment that was declined, but why?\\n\\nLabel: 21\\nText: I would like to change my pin.\\nText: How can I change my Tholepin ?\\n\\nHere is your text, please classify it into one of the above classes\\n\\nText: my credit card does not work'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enhanced_few_shot_prompt.format(text=text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "25055701-96df-46a6-a3cc-2d1d5893188d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242\n",
      "1045\n"
     ]
    }
   ],
   "source": [
    "class Prediction(BaseModel):\n",
    "    label: int = Field(description=\"classification label to a given text\")\n",
    "    @validator(\"label\")\n",
    "    def label_is_valid(cls, field): \n",
    "        if not (0 <= field <= 76):\n",
    "            raise ValueError(\"label must be an integer between 0 and 76\")\n",
    "        return field\n",
    "parser = PydanticOutputParser(pydantic_object=Prediction)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "enhanced_few_shot_prompt = FewShotPromptTemplate(\n",
    "    prefix=\"Answer the user query.\\n{format_instructions}\\n{text}\\n\",\n",
    "    example_selector=example_selector,\n",
    "    example_prompt=example_prompt,\n",
    "    suffix=\"Text: {text}\",\n",
    "    input_variables=[\"text\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()}\n",
    ")\n",
    "\n",
    "retry_parser = RetryOutputParser.from_llm(parser=parser, llm=llm, max_retries=3)\n",
    " \n",
    "completion_chain = enhanced_few_shot_prompt | llm\n",
    "\n",
    "main_chain = RunnableParallel(\n",
    "    completion=completion_chain, prompt_value=enhanced_few_shot_prompt\n",
    ") | RunnableLambda(lambda x: retry_parser.parse_with_prompt(**x))\n",
    "\n",
    "\n",
    "answer: Prediction = main_chain.invoke({\"text\": text})\n",
    "input_tokens_length, prompt_length = get_prompt_length(llm, enhanced_few_shot_prompt.format(text=text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6adf6b19-5e70-4e32-9690-9c12116cce31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "242"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tokens_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7d732c48-3a56-4f21-adcd-f2709f6bc2ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Answer the user query.\\nThe output should be formatted as a JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output schema:\\n```\\n{\"properties\": {\"label\": {\"title\": \"Label\", \"description\": \"classification label to a given text\", \"type\": \"integer\"}}, \"required\": [\"label\"]}\\n```\\nWhy does my credit card not work?\\n\\n\\nText: Can you tell me why my card keeps getting declined every time I try to use it?\\nLabel: 25\\n\\nText: why couldn\\'t I use my card at a store?\\nLabel: 25\\n\\nText: Why does my credit card not work?'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enhanced_few_shot_prompt.format(text=text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ae7812-2ba2-427e-b6f8-0b13746a40bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02fef2e4-9020-4c2b-83b9-ada0855653e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7fa9abd6-0b36-413b-861b-8e6b97b58a07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"Input to PromptTemplate is missing variables {'completion'}.  Expected: ['completion', 'prompt'] Received: ['prompt', 'input']\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m/pfs/data5/home/kit/anthropomatik/yy5819/llm_classification/venv/lib64/python3.9/site-packages/langchain_core/output_parsers/json.py:66\u001b[0m, in \u001b[0;36mJsonOutputParser.parse_result\u001b[0;34m(self, result, partial)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 66\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparse_json_markdown\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m JSONDecodeError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/pfs/data5/home/kit/anthropomatik/yy5819/llm_classification/venv/lib64/python3.9/site-packages/langchain_core/utils/json.py:147\u001b[0m, in \u001b[0;36mparse_json_markdown\u001b[0;34m(json_string, parser)\u001b[0m\n\u001b[1;32m    146\u001b[0m         json_str \u001b[38;5;241m=\u001b[39m match\u001b[38;5;241m.\u001b[39mgroup(\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m--> 147\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_parse_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson_str\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparser\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparser\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/pfs/data5/home/kit/anthropomatik/yy5819/llm_classification/venv/lib64/python3.9/site-packages/langchain_core/utils/json.py:160\u001b[0m, in \u001b[0;36m_parse_json\u001b[0;34m(json_str, parser)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;66;03m# Parse the JSON string into a Python dictionary\u001b[39;00m\n\u001b[0;32m--> 160\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson_str\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/pfs/data5/home/kit/anthropomatik/yy5819/llm_classification/venv/lib64/python3.9/site-packages/langchain_core/utils/json.py:120\u001b[0m, in \u001b[0;36mparse_partial_json\u001b[0;34m(s, strict)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;66;03m# If we got here, we ran out of characters to remove\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;66;03m# and still couldn't parse the string as JSON, so return the parse error\u001b[39;00m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;66;03m# for the original string.\u001b[39;00m\n\u001b[0;32m--> 120\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrict\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib64/python3.9/json/__init__.py:359\u001b[0m, in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    358\u001b[0m     kw[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparse_constant\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m parse_constant\n\u001b[0;32m--> 359\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib64/python3.9/json/decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;124;03mcontaining a JSON document).\u001b[39;00m\n\u001b[1;32m    335\u001b[0m \n\u001b[1;32m    336\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 337\u001b[0m obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    338\u001b[0m end \u001b[38;5;241m=\u001b[39m _w(s, end)\u001b[38;5;241m.\u001b[39mend()\n",
      "File \u001b[0;32m/usr/lib64/python3.9/json/decoder.py:355\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m--> 355\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpecting value\u001b[39m\u001b[38;5;124m\"\u001b[39m, s, err\u001b[38;5;241m.\u001b[39mvalue) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    356\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj, end\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mOutputParserException\u001b[0m                     Traceback (most recent call last)",
      "File \u001b[0;32m/pfs/data5/home/kit/anthropomatik/yy5819/llm_classification/venv/lib64/python3.9/site-packages/langchain/output_parsers/retry.py:90\u001b[0m, in \u001b[0;36mRetryOutputParser.parse_with_prompt\u001b[0;34m(self, completion, prompt_value)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 90\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcompletion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m OutputParserException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/pfs/data5/home/kit/anthropomatik/yy5819/llm_classification/venv/lib64/python3.9/site-packages/langchain_core/output_parsers/pydantic.py:64\u001b[0m, in \u001b[0;36mPydanticOutputParser.parse\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparse\u001b[39m(\u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m TBaseModel:\n\u001b[0;32m---> 64\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/pfs/data5/home/kit/anthropomatik/yy5819/llm_classification/venv/lib64/python3.9/site-packages/langchain_core/output_parsers/json.py:72\u001b[0m, in \u001b[0;36mJsonOutputParser.parse\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparse\u001b[39m(\u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m---> 72\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mGeneration\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/pfs/data5/home/kit/anthropomatik/yy5819/llm_classification/venv/lib64/python3.9/site-packages/langchain_core/output_parsers/pydantic.py:60\u001b[0m, in \u001b[0;36mPydanticOutputParser.parse_result\u001b[0;34m(self, result, partial)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparse_result\u001b[39m(\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;28mself\u001b[39m, result: List[Generation], \u001b[38;5;241m*\u001b[39m, partial: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     59\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m TBaseModel:\n\u001b[0;32m---> 60\u001b[0m     json_object \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parse_obj(json_object)\n",
      "File \u001b[0;32m/pfs/data5/home/kit/anthropomatik/yy5819/llm_classification/venv/lib64/python3.9/site-packages/langchain_core/output_parsers/json.py:69\u001b[0m, in \u001b[0;36mJsonOutputParser.parse_result\u001b[0;34m(self, result, partial)\u001b[0m\n\u001b[1;32m     68\u001b[0m msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid json output: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 69\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m OutputParserException(msg, llm_output\u001b[38;5;241m=\u001b[39mtext) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;31mOutputParserException\u001b[0m: Invalid json output: Here is the output in the required JSON format:\n\n{\"label\": 25}",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmain_chain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mquery\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/pfs/data5/home/kit/anthropomatik/yy5819/llm_classification/venv/lib64/python3.9/site-packages/langchain_core/runnables/base.py:2507\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   2505\u001b[0m             \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m step\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   2506\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2507\u001b[0m             \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2508\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[1;32m   2509\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/pfs/data5/home/kit/anthropomatik/yy5819/llm_classification/venv/lib64/python3.9/site-packages/langchain_core/runnables/base.py:3985\u001b[0m, in \u001b[0;36mRunnableLambda.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   3983\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Invoke this runnable synchronously.\"\"\"\u001b[39;00m\n\u001b[1;32m   3984\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunc\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m-> 3985\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_with_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3986\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_invoke\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3987\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3988\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3989\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3990\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3991\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3992\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m   3993\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot invoke a coroutine function synchronously.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3994\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUse `ainvoke` instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3995\u001b[0m     )\n",
      "File \u001b[0;32m/pfs/data5/home/kit/anthropomatik/yy5819/llm_classification/venv/lib64/python3.9/site-packages/langchain_core/runnables/base.py:1599\u001b[0m, in \u001b[0;36mRunnable._call_with_config\u001b[0;34m(self, func, input, config, run_type, **kwargs)\u001b[0m\n\u001b[1;32m   1595\u001b[0m     context \u001b[38;5;241m=\u001b[39m copy_context()\n\u001b[1;32m   1596\u001b[0m     context\u001b[38;5;241m.\u001b[39mrun(_set_config_context, child_config)\n\u001b[1;32m   1597\u001b[0m     output \u001b[38;5;241m=\u001b[39m cast(\n\u001b[1;32m   1598\u001b[0m         Output,\n\u001b[0;32m-> 1599\u001b[0m         \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1600\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcall_func_with_variable_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1601\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1602\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1603\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1604\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1605\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1606\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m   1607\u001b[0m     )\n\u001b[1;32m   1608\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1609\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[0;32m/pfs/data5/home/kit/anthropomatik/yy5819/llm_classification/venv/lib64/python3.9/site-packages/langchain_core/runnables/config.py:380\u001b[0m, in \u001b[0;36mcall_func_with_variable_args\u001b[0;34m(func, input, config, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    378\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m accepts_run_manager(func):\n\u001b[1;32m    379\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m run_manager\n\u001b[0;32m--> 380\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/pfs/data5/home/kit/anthropomatik/yy5819/llm_classification/venv/lib64/python3.9/site-packages/langchain_core/runnables/base.py:3853\u001b[0m, in \u001b[0;36mRunnableLambda._invoke\u001b[0;34m(self, input, run_manager, config, **kwargs)\u001b[0m\n\u001b[1;32m   3851\u001b[0m                 output \u001b[38;5;241m=\u001b[39m chunk\n\u001b[1;32m   3852\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3853\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mcall_func_with_variable_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3854\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m   3855\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3856\u001b[0m \u001b[38;5;66;03m# If the output is a runnable, invoke it\u001b[39;00m\n\u001b[1;32m   3857\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, Runnable):\n",
      "File \u001b[0;32m/pfs/data5/home/kit/anthropomatik/yy5819/llm_classification/venv/lib64/python3.9/site-packages/langchain_core/runnables/config.py:380\u001b[0m, in \u001b[0;36mcall_func_with_variable_args\u001b[0;34m(func, input, config, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    378\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m accepts_run_manager(func):\n\u001b[1;32m    379\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m run_manager\n\u001b[0;32m--> 380\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[13], line 26\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     20\u001b[0m retry_parser \u001b[38;5;241m=\u001b[39m RetryOutputParser\u001b[38;5;241m.\u001b[39mfrom_llm(parser\u001b[38;5;241m=\u001b[39mparser, llm\u001b[38;5;241m=\u001b[39mllm, max_retries\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m     22\u001b[0m completion_chain \u001b[38;5;241m=\u001b[39m prompt \u001b[38;5;241m|\u001b[39m llm\n\u001b[1;32m     24\u001b[0m main_chain \u001b[38;5;241m=\u001b[39m RunnableParallel(\n\u001b[1;32m     25\u001b[0m     completion\u001b[38;5;241m=\u001b[39mcompletion_chain, prompt_value\u001b[38;5;241m=\u001b[39mprompt\n\u001b[0;32m---> 26\u001b[0m ) \u001b[38;5;241m|\u001b[39m RunnableLambda(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[43mretry_parser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse_with_prompt\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     29\u001b[0m answer: Prediction \u001b[38;5;241m=\u001b[39m main_chain\u001b[38;5;241m.\u001b[39minvoke({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m\"\u001b[39m: text})\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m#answer_final: str = answer.label\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m#reasoning_final: str = answer.reasoning\u001b[39;00m\n",
      "File \u001b[0;32m/pfs/data5/home/kit/anthropomatik/yy5819/llm_classification/venv/lib64/python3.9/site-packages/langchain/output_parsers/retry.py:103\u001b[0m, in \u001b[0;36mRetryOutputParser.parse_with_prompt\u001b[0;34m(self, completion, prompt_value)\u001b[0m\n\u001b[1;32m     97\u001b[0m                 completion \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretry_chain\u001b[38;5;241m.\u001b[39mrun(\n\u001b[1;32m     98\u001b[0m                     prompt\u001b[38;5;241m=\u001b[39mprompt_value\u001b[38;5;241m.\u001b[39mto_string(),\n\u001b[1;32m     99\u001b[0m                     completion\u001b[38;5;241m=\u001b[39mcompletion,\n\u001b[1;32m    100\u001b[0m                     error\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mrepr\u001b[39m(e),\n\u001b[1;32m    101\u001b[0m                 )\n\u001b[1;32m    102\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 103\u001b[0m                 completion \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mretry_chain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[43m                    \u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt_value\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_string\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[43m                        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    107\u001b[0m \u001b[43m                    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[43m                \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m OutputParserException(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to parse\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/pfs/data5/home/kit/anthropomatik/yy5819/llm_classification/venv/lib64/python3.9/site-packages/langchain_core/runnables/base.py:2505\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   2501\u001b[0m config \u001b[38;5;241m=\u001b[39m patch_config(\n\u001b[1;32m   2502\u001b[0m     config, callbacks\u001b[38;5;241m=\u001b[39mrun_manager\u001b[38;5;241m.\u001b[39mget_child(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseq:step:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2503\u001b[0m )\n\u001b[1;32m   2504\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2505\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2506\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2507\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m step\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "File \u001b[0;32m/pfs/data5/home/kit/anthropomatik/yy5819/llm_classification/venv/lib64/python3.9/site-packages/langchain_core/prompts/base.py:151\u001b[0m, in \u001b[0;36mBasePromptTemplate.invoke\u001b[0;34m(self, input, config)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtags:\n\u001b[1;32m    150\u001b[0m     config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtags\n\u001b[0;32m--> 151\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_with_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    152\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_format_prompt_with_error_handling\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrun_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprompt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/pfs/data5/home/kit/anthropomatik/yy5819/llm_classification/venv/lib64/python3.9/site-packages/langchain_core/runnables/base.py:1599\u001b[0m, in \u001b[0;36mRunnable._call_with_config\u001b[0;34m(self, func, input, config, run_type, **kwargs)\u001b[0m\n\u001b[1;32m   1595\u001b[0m     context \u001b[38;5;241m=\u001b[39m copy_context()\n\u001b[1;32m   1596\u001b[0m     context\u001b[38;5;241m.\u001b[39mrun(_set_config_context, child_config)\n\u001b[1;32m   1597\u001b[0m     output \u001b[38;5;241m=\u001b[39m cast(\n\u001b[1;32m   1598\u001b[0m         Output,\n\u001b[0;32m-> 1599\u001b[0m         \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1600\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcall_func_with_variable_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1601\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1602\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1603\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1604\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1605\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1606\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m   1607\u001b[0m     )\n\u001b[1;32m   1608\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1609\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[0;32m/pfs/data5/home/kit/anthropomatik/yy5819/llm_classification/venv/lib64/python3.9/site-packages/langchain_core/runnables/config.py:380\u001b[0m, in \u001b[0;36mcall_func_with_variable_args\u001b[0;34m(func, input, config, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    378\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m accepts_run_manager(func):\n\u001b[1;32m    379\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m run_manager\n\u001b[0;32m--> 380\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/pfs/data5/home/kit/anthropomatik/yy5819/llm_classification/venv/lib64/python3.9/site-packages/langchain_core/prompts/base.py:134\u001b[0m, in \u001b[0;36mBasePromptTemplate._format_prompt_with_error_handling\u001b[0;34m(self, inner_input)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_format_prompt_with_error_handling\u001b[39m(\u001b[38;5;28mself\u001b[39m, inner_input: Dict) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m PromptValue:\n\u001b[0;32m--> 134\u001b[0m     _inner_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_input\u001b[49m\u001b[43m(\u001b[49m\u001b[43minner_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat_prompt(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_inner_input)\n",
      "File \u001b[0;32m/pfs/data5/home/kit/anthropomatik/yy5819/llm_classification/venv/lib64/python3.9/site-packages/langchain_core/prompts/base.py:126\u001b[0m, in \u001b[0;36mBasePromptTemplate._validate_input\u001b[0;34m(self, inner_input)\u001b[0m\n\u001b[1;32m    124\u001b[0m missing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_variables)\u001b[38;5;241m.\u001b[39mdifference(inner_input)\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m missing:\n\u001b[0;32m--> 126\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\n\u001b[1;32m    127\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is missing variables \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmissing\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    128\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Expected: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_variables\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    129\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Received: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(inner_input\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    130\u001b[0m     )\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m inner_input\n",
      "\u001b[0;31mKeyError\u001b[0m: \"Input to PromptTemplate is missing variables {'completion'}.  Expected: ['completion', 'prompt'] Received: ['prompt', 'input']\""
     ]
    }
   ],
   "source": [
    "main_chain.invoke({\"query\": text})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe21bbc-3921-445f-b635-d6aac7b22969",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17473d9c-38d3-4bfe-b352-9f77d7db2382",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca260a6-592b-46db-9e6a-fddbdc913ec0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ab706c-b551-44a3-8208-3f938cf9c8eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58951e01-6780-421b-b2a5-48d0aa15aaac",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 42\n",
    "n_classes = 2\n",
    "num_shots = 2\n",
    "sampled_classes = train_data['label'].sample(n_classes, random_state=random_seed).values\n",
    "train_data_sub = train_data[train_data['label'].isin(sampled_classes)]\n",
    "test_data_sub = test_data[test_data['label'].isin(sampled_classes)]\n",
    "\n",
    "def get_prompt_template(train_data_sub, num_shots):\n",
    "    prompt_template = \"Your task is to classify a given text into one of the following classes, reply ONLY with the class label: \\n\\n\"\n",
    "    for label in train_data_sub.label.unique():\n",
    "        prompt_template += f\"Label: {label}\\n\"\n",
    "        for i, row in train_data_sub[train_data_sub['label'] == label].sample(num_shots, random_state=random_seed).iterrows():\n",
    "            prompt_template += f\"Text: {row['text']}\\n\"\n",
    "            #remove row from the dataframe\n",
    "            train_data_sub = train_data_sub.drop(i)\n",
    "        prompt_template += \"\\n\"\n",
    "    prompt_template += \"Here is your text, please classify it into one of the above classes\\n\\n\"\n",
    "    return prompt_template\n",
    "\n",
    "#prompt_template = get_prompt_template(test_data_sub,num_shots)\n",
    "\n",
    "text = get_prompt_template(test_data_sub,num_shots)\n",
    "text += \"Text: my credit card does not work\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1f2a9c-f330-4aab-b50c-949cf47db7e8",
   "metadata": {},
   "source": [
    "### TODO:\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "56f36064-a51a-4ce7-8d5c-0feccef02cb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The output should be formatted as a JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output schema:\\n```\\n{\"properties\": {\"label\": {\"title\": \"Label\", \"description\": \"classification label to a given text\", \"type\": \"integer\"}}, \"required\": [\"label\"]}\\n```'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser.get_format_instructions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "566330cc-fc8a-4083-a410-89774d4b048a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:   0%|          | 0/80 [00:00<?, ?it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6259c9e661fe4fd7b7b092a940bb0d43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "/scratch/slurm_tmpdir/job_23779533/ipykernel_169380/1091179139.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_data_sub.loc[i, 'response'] = response\n",
      "Processing data:   1%|▏         | 1/80 [00:16<21:17, 16.18s/it]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:   2%|▎         | 2/80 [00:16<09:01,  6.94s/it]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:   4%|▍         | 3/80 [00:17<05:06,  3.98s/it]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:   5%|▌         | 4/80 [00:17<03:17,  2.59s/it]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:   6%|▋         | 5/80 [00:18<02:16,  1.83s/it]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:   8%|▊         | 6/80 [00:18<01:40,  1.36s/it]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:   9%|▉         | 7/80 [00:18<01:18,  1.07s/it]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:  10%|█         | 8/80 [00:19<01:03,  1.14it/s]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:  11%|█▏        | 9/80 [00:19<00:53,  1.33it/s]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:  12%|█▎        | 10/80 [00:20<00:46,  1.51it/s]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:  14%|█▍        | 11/80 [00:20<00:41,  1.66it/s]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:  15%|█▌        | 12/80 [00:21<00:38,  1.78it/s]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:  16%|█▋        | 13/80 [00:21<00:35,  1.88it/s]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:  18%|█▊        | 14/80 [00:22<00:33,  1.96it/s]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:  19%|█▉        | 15/80 [00:22<00:32,  2.01it/s]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:  20%|██        | 16/80 [00:23<00:31,  2.05it/s]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:  21%|██▏       | 17/80 [00:23<00:30,  2.07it/s]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:  22%|██▎       | 18/80 [00:24<00:29,  2.09it/s]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:  24%|██▍       | 19/80 [00:24<00:28,  2.11it/s]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:  25%|██▌       | 20/80 [00:25<00:28,  2.12it/s]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:  26%|██▋       | 21/80 [00:25<00:27,  2.12it/s]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:  28%|██▊       | 22/80 [00:25<00:27,  2.13it/s]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:  29%|██▉       | 23/80 [00:26<00:26,  2.13it/s]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:  30%|███       | 24/80 [00:26<00:26,  2.14it/s]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:  31%|███▏      | 25/80 [00:27<00:25,  2.14it/s]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:  32%|███▎      | 26/80 [00:27<00:25,  2.14it/s]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:  34%|███▍      | 27/80 [00:28<00:24,  2.14it/s]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:  35%|███▌      | 28/80 [00:28<00:24,  2.15it/s]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:  36%|███▋      | 29/80 [00:29<00:23,  2.15it/s]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:  38%|███▊      | 30/80 [00:29<00:23,  2.15it/s]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:  39%|███▉      | 31/80 [00:30<00:22,  2.15it/s]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:  40%|████      | 32/80 [00:30<00:22,  2.15it/s]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:  41%|████▏     | 33/80 [00:31<00:21,  2.15it/s]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:  42%|████▎     | 34/80 [00:31<00:21,  2.15it/s]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:  44%|████▍     | 35/80 [00:32<00:20,  2.15it/s]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:  45%|████▌     | 36/80 [00:32<00:20,  2.14it/s]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:  46%|████▋     | 37/80 [00:32<00:20,  2.14it/s]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:  48%|████▊     | 38/80 [00:33<00:19,  2.14it/s]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:  49%|████▉     | 39/80 [00:33<00:19,  2.14it/s]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:  50%|█████     | 40/80 [00:34<00:18,  2.14it/s]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:  51%|█████▏    | 41/80 [00:34<00:18,  2.14it/s]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:  52%|█████▎    | 42/80 [00:35<00:17,  2.14it/s]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:  54%|█████▍    | 43/80 [00:35<00:17,  2.14it/s]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:  55%|█████▌    | 44/80 [00:36<00:16,  2.15it/s]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:  56%|█████▋    | 45/80 [00:36<00:16,  2.15it/s]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:  57%|█████▊    | 46/80 [00:37<00:15,  2.15it/s]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:  59%|█████▉    | 47/80 [00:37<00:15,  2.14it/s]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:  60%|██████    | 48/80 [00:38<00:14,  2.15it/s]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:  61%|██████▏   | 49/80 [00:38<00:14,  2.15it/s]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:  62%|██████▎   | 50/80 [00:39<00:13,  2.15it/s]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:  64%|██████▍   | 51/80 [00:39<00:13,  2.15it/s]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:  65%|██████▌   | 52/80 [00:39<00:13,  2.15it/s]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:  66%|██████▋   | 53/80 [00:40<00:12,  2.15it/s]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:  68%|██████▊   | 54/80 [00:40<00:12,  2.15it/s]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:  69%|██████▉   | 55/80 [00:41<00:11,  2.15it/s]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:  70%|███████   | 56/80 [00:41<00:11,  2.15it/s]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:  71%|███████▏  | 57/80 [00:42<00:10,  2.15it/s]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:  72%|███████▎  | 58/80 [00:42<00:10,  2.15it/s]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:  74%|███████▍  | 59/80 [00:43<00:09,  2.14it/s]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:  75%|███████▌  | 60/80 [00:43<00:09,  2.14it/s]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:  76%|███████▋  | 61/80 [00:44<00:08,  2.14it/s]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:  78%|███████▊  | 62/80 [00:44<00:08,  2.14it/s]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:  79%|███████▉  | 63/80 [00:45<00:07,  2.14it/s]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:  80%|████████  | 64/80 [00:45<00:07,  2.14it/s]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:  81%|████████▏ | 65/80 [00:46<00:06,  2.14it/s]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:  82%|████████▎ | 66/80 [00:46<00:06,  2.15it/s]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:  84%|████████▍ | 67/80 [00:46<00:06,  2.14it/s]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:  85%|████████▌ | 68/80 [00:47<00:05,  2.14it/s]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:  86%|████████▋ | 69/80 [00:47<00:05,  2.15it/s]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:  88%|████████▊ | 70/80 [00:48<00:04,  2.15it/s]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:  89%|████████▉ | 71/80 [00:48<00:04,  2.14it/s]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:  90%|█████████ | 72/80 [00:49<00:03,  2.14it/s]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:  91%|█████████▏| 73/80 [00:49<00:03,  2.15it/s]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:  92%|█████████▎| 74/80 [00:50<00:02,  2.14it/s]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:  94%|█████████▍| 75/80 [00:50<00:02,  2.14it/s]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:  95%|█████████▌| 76/80 [00:51<00:01,  2.14it/s]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:  96%|█████████▋| 77/80 [00:51<00:01,  2.14it/s]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:  98%|█████████▊| 78/80 [00:52<00:00,  2.14it/s]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:  99%|█████████▉| 79/80 [00:52<00:00,  2.06it/s]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data: 100%|██████████| 80/80 [00:53<00:00,  1.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i, row in tqdm.tqdm(test_data_sub.iterrows(),total=len(test_data_sub), desc=\"Processing data\"):\n",
    "    prompt = prompt_template + f\"Text: {row['text']}\\n\"\n",
    "    response = llm(prompt=prompt)\n",
    "    test_data_sub.loc[i, 'response'] = response\n",
    "    #print(f\"Prompt: {prompt}\")\n",
    "    #print(f\"Response: {response}\")\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6cbd4812-0c81-46ca-b8ef-7850ec2b5280",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>Why won't my card show up on the app?</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>I would like to reactivate my card.</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Where do I link the new card?</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>I have received my card, can you help me put i...</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>How do I link a card that I already have?</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1955</th>\n",
       "      <td>The ATM at Metro bank on High St. Kensington d...</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1956</th>\n",
       "      <td>The ATM won't give back my card</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1957</th>\n",
       "      <td>I was at an ATM and it swallowed my card.</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1958</th>\n",
       "      <td>WTF??? I tried to withdraw some money at a Met...</td>\n",
       "      <td>18</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1959</th>\n",
       "      <td>What happens if my card is stuck in the ATM?</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>80 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  label response\n",
       "40                Why won't my card show up on the app?     13       13\n",
       "41                  I would like to reactivate my card.     13       13\n",
       "42                        Where do I link the new card?     13       13\n",
       "43    I have received my card, can you help me put i...     13       13\n",
       "44            How do I link a card that I already have?     13       13\n",
       "...                                                 ...    ...      ...\n",
       "1955  The ATM at Metro bank on High St. Kensington d...     18       18\n",
       "1956                    The ATM won't give back my card     18       18\n",
       "1957          I was at an ATM and it swallowed my card.     18       18\n",
       "1958  WTF??? I tried to withdraw some money at a Met...     18       13\n",
       "1959       What happens if my card is stuck in the ATM?     18       18\n",
       "\n",
       "[80 rows x 3 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8d3b4a6d-e17a-4526-9533-a5bde1b9e4c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/slurm_tmpdir/job_23779533/ipykernel_169380/3811748138.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_data_sub['response'] = test_data_sub['response'].astype(int)\n"
     ]
    }
   ],
   "source": [
    "test_data_sub['response'] = test_data_sub['response'].astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "35afdc67-0dc5-4b13-a34b-241979d318f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          13       0.83      1.00      0.91        40\n",
      "          18       1.00      0.80      0.89        40\n",
      "\n",
      "    accuracy                           0.90        80\n",
      "   macro avg       0.92      0.90      0.90        80\n",
      "weighted avg       0.92      0.90      0.90        80\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(test_data_sub['label'], test_data_sub['response']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8cc3e231-2808-48c6-9c1c-64d19e432482",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          13       0.58      0.88      0.70        40\n",
      "          18       0.75      0.38      0.50        40\n",
      "\n",
      "    accuracy                           0.62        80\n",
      "   macro avg       0.67      0.62      0.60        80\n",
      "weighted avg       0.67      0.62      0.60        80\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(test_data_sub['label'], test_data_sub['response']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b94e4a15-3b6a-4409-afb9-e0f7dca60278",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:   0%|          | 0/200 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__call__() missing 1 required keyword-only argument: 'query'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 27\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, row \u001b[38;5;129;01min\u001b[39;00m tqdm\u001b[38;5;241m.\u001b[39mtqdm(test_data_sub\u001b[38;5;241m.\u001b[39miterrows(),total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(test_data_sub), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing data\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     26\u001b[0m     prompt \u001b[38;5;241m=\u001b[39m prompt_template \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mText: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrow[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 27\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mllm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m     test_data_sub\u001b[38;5;241m.\u001b[39mloc[i, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m response\n\u001b[1;32m     29\u001b[0m test_data_sub[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m test_data_sub[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mlabel\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: __call__() missing 1 required keyword-only argument: 'query'"
     ]
    }
   ],
   "source": [
    "#iterate over the test data and generate the prompt\n",
    "#surpress warnings\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "prompt_template = get_prompt_template(test_data_sub, num_shots)\n",
    "\n",
    "n_classes_list = [5, 10, 20, 30, 40, 50, 77]\n",
    "num_shots = 5\n",
    "results = {}\n",
    "#iterate over the n_classes_list\n",
    "\n",
    "# import wandb \n",
    "# wandb.init(project=\"llm-banking77\")\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "for n_classes in n_classes_list:\n",
    "    sampled_classes = train_data['label'].sample(n_classes, random_state=random_seed).values\n",
    "    train_data_sub = train_data[train_data['label'].isin(sampled_classes)]\n",
    "    test_data_sub = test_data[test_data['label'].isin(sampled_classes)]\n",
    "    prompt_template = get_prompt_template(test_data_sub, num_shots)\n",
    "\n",
    "    for i, row in tqdm.tqdm(test_data_sub.iterrows(),total=len(test_data_sub), desc=\"Processing data\"):\n",
    "        prompt = prompt_template + f\"Text: {row['text']}\\n\"\n",
    "        response = llm(prompt=prompt)\n",
    "        test_data_sub.loc[i, 'response'] = response\n",
    "    test_data_sub['response'] = test_data_sub['response'].label.astype(int)\n",
    "    report = classification_report(test_data_sub['label'], test_data_sub['response'], output_dict=True)\n",
    "    # #print(f\"Classification Report: {classification_report}\")\n",
    "    # #save results to dictionary\n",
    "    # results[n_classes] = {}\n",
    "    results[n_classes] = {\n",
    "        'classification_report': report,\n",
    "        'prompt_template': prompt_template,\n",
    "        'context_length': len(prompt_template)\n",
    "    }\n",
    "    \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025e63a3-0120-42d6-98eb-f9172250a30b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"../data/results.json\", \"w\") as f:\n",
    "    json.dump(results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcca0297-f384-41ed-983c-c8fef0de93e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "classification_venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
